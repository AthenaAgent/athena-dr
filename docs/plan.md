# Plan

## Objective

The objective is to train a comprehensive deep research agent system in the 8B parameter range (Usuing Qwen-3 8B or a similar model as the base model) that is competitive with single agent models of similar size on the following benchmarks:
1. [Frames](./benchmarks.md#frames-factuality-retrieval-and-reasoning-measurement-set) benchmark (> 63.3 which is achieved by SFR-DR-8B).
2. [BrowseComp](./benchmarks.md#browsecomp-a-benchmark-for-browsing-agents) benchmark (> 0.434 which is achieved by Tongyi-DR-30B).
3. [Xbench DeepSearch](./benchmarks.md#xbench-deepsearch) benchmark (> 75.0 which is achieved by Tongyi-DR-30B).
4. [WebWalkerQA](https://huggingface.co/datasets/callanwu/WebWalkerQA) benchmark (> 72.2 which is achieved by Tongyi-DR-30B).
5. [SimpleQA](./benchmarks.md#simpleqa) benchmark (> 98.6 which is achieved by Tongyi-DR-30B).

## Approach

The approach involves 2 steps:

1. **Supervised Fine-tuning**: Train the model on high-quality instruction-following and reasoning traces generated by putting strong frontier models in deep research agent scaffolds. If we decide to proceed with Tulu-DR scaffold, we can leverage their [SFT dataset](https://huggingface.co/datasets/rl-research/dr-tulu-sft-data) as a starting point. This dataset included rejection sampled traces from [OpenScholar](https://huggingface.co/datasets/allenai/openscilm_queries), [Search Arena](https://huggingface.co/datasets/lmarena-ai/search-arena-24k), and short-form QA datasets inclduing [WebWalker-Silver](https://huggingface.co/datasets/callanwu/WebWalkerQA), [TaskCraft](https://huggingface.co/datasets/PersonalAILab/TaskCraft), [PopQA](https://huggingface.co/datasets/akariasai/PopQA) and [TyDiQA (English)](https://github.com/google-research-datasets/tydiqa). The dataset doesn't consist of prompts from [MegaScience](https://huggingface.co/datasets/MegaScience/MegaScience), [HotpotQA](https://huggingface.co/datasets/hotpotqa/hotpot_qa), and [ScholarQA](https://allenai.org/blog/ai2-scholarqa).

2. **Reinforcement Learning Fine-tuning**